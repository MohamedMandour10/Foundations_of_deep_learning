{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Closed Form Evaluation of the Gradient\n",
        "The derivative can be also calculated in a closed form solution.\n",
        "\n",
        "$\\frac{\\partial \\mathbb{L}}{\\partial x_p} = -\\frac{1}{N} \\sum_{i=0}^{N-1}((x_i - x_p)^2 + (y_i - y_p)^2)^\\frac{-1}{2}(x_i - x_p)$\n",
        "<br>\n",
        "$\\frac{\\partial \\mathbb{L}}{\\partial y_p} = -\\frac{1}{N} \\sum_{i=0}^{N-1}((x_i - x_p)^2 + (y_i - y_p)^2)^\\frac{-1}{2}(y_i - y_p)$\n",
        "\n",
        "Note that this approach will turn to enable more efficient solution as we do backpropagation."
      ],
      "metadata": {
        "id": "FRYQSUDt7h1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how to calculate $\\frac{\\partial \\mathbb{L}}{\\partial x_p}$:\n",
        "<br><br>\n",
        "$$\n",
        "\\mathbb{L} = \\frac{1}{N} \\sum_{i=0}^{N-1}[(x_{i} - x_{p})^{2} + (y_{i} - y_{p})^{2}]^{\\frac{1}{2}}\n",
        "\\\\\n",
        "\\mathbb{L} = C \\sum_{i=0}^{N-1}{\\mathbb{L}(x_i, y_i)}\n",
        "\\\\\n",
        "where\\; C = \\frac{1}{N}\n",
        "$$\n",
        "<br>\n",
        "$$\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial x_p} = C \\sum_{i=0}^{N-1}{\\frac{\\partial \\mathbb{L}}{\\partial x_p}(x_i, y_i)}\n",
        "$$\n",
        "<br>\n",
        "$$\n",
        "Required\\; is\\; to\\; get: \\;\\;\\frac{\\partial \\mathbb{L}}{\\partial x_p},\\;\\; \\frac{\\partial \\mathbb{L}}{\\partial y_p}\n",
        "$$\n",
        "<br>\n",
        "$$\n",
        "\\mathbb{L} (x_i, y_i) = [(x_{i} - x_{p})^{2} + (y_{i} - y_{p})^{2}]^{\\frac{1}{2}}\n",
        "$$\n",
        "<br>\n",
        "$$\n",
        "Let \\;\\; I_x = x_i - x_p, \\;\\; I_y = y_i - y_p\n",
        "\\\\\n",
        "g_x = I_x^2, \\;\\; g_y = I_y^2\n",
        "\\\\\n",
        "M = g_x + g_y\n",
        "\\\\\n",
        "\\mathbb{L} = M^\\frac{1}{2}\n",
        "$$\n",
        "<br>\n",
        "$$\n",
        "\\frac{\\partial I_x}{\\partial x_p} = -1,\n",
        "\\;\\; \\frac{\\partial I_y}{\\partial x_p} = 0\n",
        "\\\\\n",
        "\\frac{\\partial g_x}{\\partial I_x} = 2 I_x,\n",
        "\\;\\; \\frac{\\partial g_y}{\\partial I_y} = 2 I_y\n",
        "\\\\\n",
        "\\frac{\\partial M}{\\partial g_x} = 1,\n",
        "\\;\\; \\frac{\\partial M}{\\partial g_y} = 1\n",
        "\\\\\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial M} = \\frac{1}{2} M^\\frac{-1}{2}\n",
        "$$\n",
        "<br>\n",
        "$$\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial g_x} = \\frac{\\partial \\mathbb{L}}{\\partial M}\\;\\;.\\;\\frac{\\partial M}{\\partial g_x}\n",
        "\\\\\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial g_x} = \\frac{1}{2} M^\\frac{-1}{2} \\;. \\; 1\n",
        "\\\\\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial I_x} = \\frac{\\partial \\mathbb{L}}{\\partial g_x}\\;\\;.\\;\\frac{\\partial g_x}{\\partial I_x}\n",
        "\\\\\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial g_x} = \\frac{1}{2} M^\\frac{-1}{2} \\;. \\; 2I_x\n",
        "\\\\\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial g_x} = M^\\frac{-1}{2} \\; . \\; I_x\n",
        "\\\\\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial x} = \\frac{\\partial \\mathbb{L}}{\\partial I_x}\\;\\;.\\;\\frac{\\partial I_x}{\\partial x_p}\n",
        "\\\\\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial x_p} = M^\\frac{-1}{2} \\; . \\; I_x \\; . \\; -1\n",
        "$$\n",
        "<br>\n",
        "$$\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial x_p} = -M^\\frac{-1}{2} \\; . \\; I_x\n",
        "\\\\\n",
        "\\frac{\\partial \\mathbb{L}}{\\partial x_p} = -((x_i - x_p)^2 + (y_i - y_p)^2)^\\frac{-1}{2} \\;\\; . \\;\\; (x_i - x_p) = \\frac{\\partial \\mathbb{L}(x_i, y_i)}{\\partial g_x}\n",
        "$$"
      ],
      "metadata": {
        "id": "W0j6sQzS7jgk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuCa1WvV7ahB"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Calculate the gradient of a set of points with respect to a given point.\n",
        "\n",
        "Parameters:\n",
        "    x_i (list or array-like): A list or array containing the x-coordinates of a set of points.\n",
        "    y_i (list or array-like): A list or array containing the y-coordinates of the same set of points as x_i.\n",
        "    x_p (float, optional): The x-coordinate of a point (not necessarily one of the points in x_i and y_i) at which to compute the gradient. Defaults to 5.\n",
        "    y_p (float, optional): The y-coordinate of the same point as x_p. Defaults to 5.\n",
        "\n",
        "Returns:\n",
        "    A tuple of two floats representing the x-component and y-component of the gradient.\n",
        "\"\"\"\n",
        "def calc_grad(x_i, y_i, x_p = 5, y_p = 5):\n",
        "  # Let's initialize the summation in x and y to 0\n",
        "  sum_x, sum_y = 0, 0\n",
        "  n = len(x_i)\n",
        "\n",
        "  \"\"\"\n",
        "  zip() method is used here to return (xi, yi)\n",
        "  for each value in the x_i, y_i lists\n",
        "  Know more about zip() method:\n",
        "  https://www.geeksforgeeks.org/zip-in-python/\n",
        "  \"\"\"\n",
        "  for x, y in zip(x_i, y_i):\n",
        "    \"\"\"\n",
        "    Let's calculate the dloss/dxp and dloss/dy, and sum it to sum_x, sum_y\n",
        "    First we will calculate the inverted square root value\n",
        "    and we will multiply it by (xi - xp) and (yi - yp)\n",
        "    to update the x, y weights\n",
        "    \"\"\"\n",
        "    inv_sqrt = ((x - x_p) ** 2 + (y - y_p) ** 2) ** (-0.5)\n",
        "    sum_x += inv_sqrt * (x - x_p)\n",
        "    sum_y += inv_sqrt * (y - y_p)\n",
        "  \n",
        "  # We will return the gradient of x, y by diving the summation by -size\n",
        "  return -sum_x/n, -sum_y/n\n",
        "\n",
        "# Try to change these values and watch what will happen :)\n",
        "x_p, y_p = 5, 5\n",
        "# Try to change this value and watch what will happen :)\n",
        "H_NEW = 0.1\n",
        "grad_x, grad_y = calc_grad(data_x, data_y)\n",
        "\n",
        "# Calculate the dloss/dx and dloss/dy to compare\n",
        "dloss_dx = (loss(data_x, data_y, x_p + H_NEW, y_p) - loss(data_x, data_y, x_p, y_p)) / H_NEW\n",
        "dloss_dy = (loss(data_x, data_y, x_p, y_p + H_NEW) - loss(data_x, data_y, x_p, y_p)) / H_NEW\n",
        "\n",
        "print(f\"Closed Form: ({grad_x}, {grad_y})\")\n",
        "print(f\"Original Form: ({dloss_dx}, {dloss_dy})\")\n"
      ]
    }
  ]
}