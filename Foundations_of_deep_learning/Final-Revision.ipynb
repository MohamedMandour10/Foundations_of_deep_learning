{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO/2uYUXIuukFPVF3TsNG5D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ${\\color{salmon} {Lecture2: \\text {Gradient Descent}}}$\n"],"metadata":{"id":"0bLJC-Hovb0-"}},{"cell_type":"markdown","source":["## $\\text {recommneded video: }$ [INTRO to ML](https://youtu.be/VvzKd7wdjMM?t=4031)"],"metadata":{"id":"rjlDIwDQlkhG"}},{"cell_type":"markdown","source":["##  $$\\text{ ðŸ“˜ A. Important Terminologies} $$"],"metadata":{"id":"TBDAJW69yhJ5"}},{"cell_type":"markdown","source":["* ## **Gradient Descent**:\n","Gradient descent is an iterative optimization algorithm used to minimize a function by iteratively adjusting the parameters in the direction of the steepest descent of the function.\n","\n","<img src=\"https://i.ibb.co/Jcdc1nr/631731-P7z2-BKhd0-R-9uyn9-Th-Das-A.png\" width=\"50%\" > \n","\n","****************************\n","* ## **Cost Function**: \n","Also known as the loss function, the cost function measures the error or discrepancy \"Difference\" between the ${\\color{skyblue} {predicted}}$ values and the ${\\color{skyblue} {actual}}$ values. In gradient descent, the goal is to minimize the cost function. Examples:\n","\n","$$\\;\\mathbb{L} = \\frac{1}{N} \\sum_{i=0}^{N-1}[(x_{i} - x_{p})^{2} + (y_{i} - y_{p})^{2}]^{\\frac{1}{2}}$$\n","\n","$$\\;\\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - f_{\\theta}(x_i))^2$$\n","\n","where $y_i$ is actual value and $f_{\\theta}(x_i)$ is predicted one\n","\n","\n","****************************\n","* ## **Learning Rate**:\n","The learning rate is a hyperparameter that determines the step size or the amount by which the parameters are updated in each iteration of gradient descent. It controls the speed at which the algorithm converges to the optimal solution, refered as ${\\color{skyblue} {\\text {DELTA}}}$ in lectures\n","![grad](https://i.ibb.co/MkDFRF2/Screen-Shot-2018-02-24-at-11-47-09-AM.png)\n","\n","****************************\n","* ## **Convergence**:\n","Convergence refers to the point at which the optimization algorithm reaches a stable or optimal solution. In gradient descent, convergence is achieved when the parameters no longer significantly change with each iteration.\n","![grad](https://i.ibb.co/cxJP2Qs/Screenshot-2023-05-27-000232.png)\n","****************************\n","* ## **Epochs**:\n"," is a hyperparameter that determines how many times the algorithm iterates over the entire dataset. It is essential to find an appropriate balance, as using too few epochs might not allow the model to learn adequately, while using too many epochs can lead to overfitting, where the model becomes too specialized to the training data and performs poorly on unseen data."],"metadata":{"id":"Hi9zqe40y1SG"}},{"cell_type":"markdown","source":["## $$\\text {B. Steps} $$"],"metadata":{"id":"PWxzQQyK3Kji"}},{"cell_type":"markdown","source":["### 1- Derive Dataset and Assume model's parameters \"Suitable\", which are:\n","$$x_p , y_p , EPOCHS , DELTA $$\n","\n","\n","### 2- Write $\\text{Loss function}$, which is in our case least Euclidean Distance \n","$$\\;\\mathbb{L} = \\frac{1}{N} \\sum_{i=0}^{N-1}[(x_{i} - x_{p})^{2} + (y_{i} - y_{p})^{2}]^{\\frac{1}{2}}$$\n","```python\n","    l = (1/len(data_x))* sum([root((x_i-x_p)**2+(y_i-y_p)**2) for x_i , y_i in zip(data_x, data_y)])\n","```\n","\n","### 3- Compute Optimaization loop, which consists of:\n","* calculation of loss function for each loop\n","```python\n","       losses.append(loss(x_p,y_p))\n","```\n","\n","* Calc derivate of loss w.r.t each parameter\n","```python\n","      dl_dx = (loss(x_p + H , y_p) - loss(x_p,y_p)) /H\n","      dl_dy = ( loss(x_p  , y_p + H) - loss(x_p,y_p) ) /H\n","```\n","\n","* Update $ x_p $ and  $ y_p $ throught each iteration\n","```python\n","        x_p -= DELTA *dl_dx\n","        y_p -= DELTA *dl_dy\n","```"],"metadata":{"id":"IOaqeA673fb6"}},{"cell_type":"markdown","source":["# ${\\color{salmon} {Lecture3: \\text {Backpropagation}}}$"],"metadata":{"id":"imoyKdPI5RWv"}},{"cell_type":"markdown","source":["## $$\\text{ A. Important Terminologies} $$"],"metadata":{"id":"MHhCQCZI8Dv6"}},{"cell_type":"markdown","source":["* ## **Backpropagation**:\n"," is a technique used in the training of neural networks. It involves the calculation of ${\\color{skyblue} {gradients}}$ that indicate how the network's weights and biases should be adjusted to ${\\color{skyblue} {minimize}}$ the difference between predicted outputs and the actual desired outputs. By propagating the errors ${\\color{skyblue} {backward}}$ from the output layer to the input layer, the algorithm determines how each weight contributes to the overall error. This information is then used to ${\\color{skyblue} {update}}$ the parameters of the network, gradually improving its performance over time. In simple terms, backpropagation is the mechanism that allows a neural network to learn from its mistakes and adjust its internal parameters to make better predictions.\n","\n","\n","<img src=\"https://editor.analyticsvidhya.com/uploads/18870backprop2.png\" width=\"50%\" > \n","\n","****************************\n"," * ## **Chain Rule**:\n"," It enables the calculation of gradients by decomposing the overall derivative into a sequence of smaller derivatives in each layer of the network.\n","\n","*******************************\n"," * ## **Fowrad path** :\n"," input data is passed through the layers of a neural network, starting from the input layer and moving towards the output layer. At each layer, the input is transformed by multiplying it with weights, summing the results, and applying an activation function. This process is repeated for each layer until the final output is obtained. \n","\n","\n","\\[\n","\\begin{align*}\n","\\text{Input:} \\quad & x_1, x_2 \\text{ (input values)} \\\\\n","\\text{Output:} \\quad & y \\text{ (predicted output)} \\\\\n","\\\\\n","\\text{Calculate weighted sum:} \\\\\n","z &= w_1 \\cdot x_1 + w_2 \\cdot x_2 + b \\\\\n","\\\\\n","\\text{Apply activation function:} \\\\\n","y &= \\sigma(z) \\\\\n","\\end{align*}\n","\\]\n","\n","***********************\n","### $\\text{Ps: I will talk about activation function in lecture 6}$"],"metadata":{"id":"nJCpPoTU7CFN"}},{"cell_type":"markdown","source":["# ${\\color{salmon} {Lecture4: \\text {Stochastic gradient descent}}}$"],"metadata":{"id":"b8Fukiz8Bk8k"}},{"cell_type":"markdown","source":["## $$\\text{ A. Important Terminologies} $$"],"metadata":{"id":"NqiefESt8Fyy"}},{"cell_type":"markdown","source":["* ## **Stochastic Gradient Descent(SGD)**:\n"," It updates the model parameters based on the gradients computed on small random subsets of the training data, called mini-batches, rather than using the entire dataset.\n"," * ### Importance:\n","   1. Efficiency with large datasets\n","   2. Faster convergence\n","*******************************************\n","\n","* ## **Mini-Batch**: \n"," A mini-batch is a subset of the training data that is randomly sampled from the full dataset. In SGD, instead of using the entire dataset to compute the gradients, the model parameters are updated based on the gradients calculated on a mini-batch. The size of the mini-batch is typically smaller than the total training dataset but larger than a single training example."],"metadata":{"id":"InpUP_iWC31K"}},{"cell_type":"markdown","source":["|                    | Gradient Descent (GD)              | Stochastic Gradient Descent (SGD)   |\n","|--------------------|------------------------------------|-------------------------------------|\n","| Dataset Size       | Suitable for small datasets         | Suitable for large datasets          |\n","| Efficiency         | Computationally expensive           | Computationally efficient            |\n","| Gradient Update    | Uses entire dataset for each update | Uses mini-batches for each update    |\n","| Convergence Speed  | Slower convergence                  | Faster convergence                   |\n","| Noise in Gradients | Less noise due to full dataset      | More noise due to mini-batch sampling|\n","\n"],"metadata":{"id":"_31nXa6-Enlg"}},{"cell_type":"markdown","source":["##  ${\\color{skyblue} {\\text {Important notes}}} $\n","\n","* #### Higher learning rates can introduce more noise, especially if the updates are too large and cause the optimization process to become unstable.\n","\n","* #### Smaller batch sizes can also introduce more noise, as the updates are based on a smaller subset of the data and may not accurately represent the overall gradient of the objective function.\n","\n","#### In practice, finding the right balance between learning rate and batch size is crucial. Too high of a learning rate or too small of a batch size can lead to noisy updates and hinder the convergence of the training process. It is often necessary to tune these hyperparameters to achieve optimal performance in training a model using SGD."],"metadata":{"id":"zT-c4HGo0p33"}},{"cell_type":"markdown","source":["## $$\\text {B. Steps} $$"],"metadata":{"id":"3XpIemLLxXlI"}},{"cell_type":"markdown","source":["## 1- Shuffle and Partition the Dataset\n","```python\n","        BATCH_SZ = 4\n","        N = len(data_x)\n","        NUM_BATCH =   ceil (N/BATCH_SZ)\n","        INDICIS = [i for i in range(len(data_x))]\n","        Sampler = Random(x=SEED)\n","        Sampler.shuffle(INDICIS)\n","```\n","\n","## 2- Iterate over Mini-Batches\n","``` python\n","        for BATCH_IND in range(NUM_BATCH):\n","          batch_indices = INDICIS[BATCH_SZ * BATCH_IND : BATCH_SZ * (BATCH_IND+1) ]\n","          batch_x = [data_x[i] for i in batch_indices]\n","          batch_y = [data_y[i] for i in batch_indices]\n","```\n","## 3- Calculate gradient upoun each batch\n","``` python\n","        grad_x_sgd , grad_y_sgd = calc_grad(x_p_sgd , y_p_sgd, batch_x ,batch_y) \n","```\n","## 4- update parameters to converge\n","``` python\n","         x_p_sgd -= DELTA * grad_x_sgd\n","         y_p_sgd -= DELTA * grad_y_sgd\n","         losses_sgd.append(loss(x_p_sgd , y_p_sgd , data_x , data_y))\n","```"],"metadata":{"id":"Kp96TbCmx7L4"}},{"cell_type":"markdown","source":["# ${\\color{salmon} {Lecture5: \\text {PyTorch}}}$"],"metadata":{"id":"z99nBDmJG_kH"}},{"cell_type":"markdown","source":["## $\\text {recommneded video: }$ [NumPy Library](https://youtu.be/LHBE6Q9XlzI?t=34571)"],"metadata":{"id":"5YeGHjIAmoY1"}},{"cell_type":"markdown","source":["## $$\\text{ A. Important Terminologies} $$"],"metadata":{"id":"GgRJTkWc8HlK"}},{"cell_type":"markdown","source":["* ## **Strides**:\n","parameter that determines the step size or the number of elements to skip while traversing a tensor\n","<img src=\"https://pirunita.github.io/assets/img/pytorch/3_2.png\" width=\"70%\" height=\"70%\">\n","\n","  * Dim 0 ----> Vertical \n","  * Dim 1 ----> Horizontal\n","\n","*********************************\n","\n","* ## **Broadcasting**:\n","allows for performing element-wise operations between tensors of different shapes, as long as certain compatibility rules are met."],"metadata":{"id":"csx4oOJ_HWNn"}},{"cell_type":"code","source":["# example of broadcasting\n","import torch\n","\n","# Create a tensor with shape (3, 3)\n","a = torch.tensor([[1, 2, 3],\n","                  [4, 5, 6],\n","                  [7, 8, 9]])\n","\n","# Create a tensor with shape (1, 3)\n","b = torch.tensor([[10, 20, 30]])\n","\n","# Perform element-wise multiplication\n","c = a * b\n","\n","print(c)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cs2AjjcJJeZz","executionInfo":{"status":"ok","timestamp":1685139654891,"user_tz":-180,"elapsed":8586,"user":{"displayName":"Muhamed El-Sayed","userId":"18011768401017469465"}},"outputId":"28cd3754-d100-479d-abdf-3cd56ed74d1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 10,  40,  90],\n","        [ 40, 100, 180],\n","        [ 70, 160, 270]])\n"]}]},{"cell_type":"markdown","source":["# ${\\color{salmon} {Lecture6: \\text {Supervised learning}}}$"],"metadata":{"id":"35lTQtdhKR4i"}},{"cell_type":"markdown","source":["## $\\text {recommneded video: }$ [Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ Backpropagation](https://www.youtube.com/watch?v=7lIF45KvGy4)"],"metadata":{"id":"zD9ECuM9mUe-"}},{"cell_type":"markdown","source":["## $$\\text{ A. Important Terminologies} $$"],"metadata":{"id":"JC5FMHEg8JZK"}},{"cell_type":"markdown","source":["* ## **SOP**:\n","stands for \"Sum Of Products\", which refers to the weighted sum of the inputs to a neuron in a neural network. The inputs are multiplied by their corresponding weights and then summed together with a bias term, resulting in a single scalar value that represents the total input to the neuron.\n","\\\n","\\begin{align*}\n","\\text{Calculate weighted sum:} \\\\\n","a_0 &= w_{(0,0)} \\cdot x_1 + w_{(0,1)} \\cdot x_2 + b_1 \\\\\n","a_1 &= w_{(1,0)} \\cdot x_1 + w_{(1,1)} \\cdot x_2 + b_2\n","\\\\\n","\\text {weights is 2x2 matrix} \\\\\n","\\text {biases is 1x2 matrix}\n","\\end{align*}\n","\n","*********************************************\n","* ## **Activation function**:\n","The output of the SOP is then passed through an activation function, which is typically a non-linear function that introduces non-linearity into the neural network. This is important because without non-linear activation functions, a neural network would simply be a linear model, and would not be able to learn complex patterns or relationships in the data, Examples:\n","\n","    1. ## Softmax \n","      * an activation function that is typically used for multi-class classification problems. It transforms the output of a neural network into a probability distribution over the classes, where the sum of the probabilities is 1. The output of the Softmax function is always between 0 and 1.\n","      $$\\text{softmax(z_i) = exp(z_i) / sum(exp(z_j))}$$\n","\n","    2. ## Tanh\n","      * an activation function that is typically used in hidden layers of neural networks. It maps the input to a range between -1 and 1. It is similar to the sigmoid function, but it has a steeper gradient around 0.\n","      $$\\text{tanh(z) = (exp(z) - exp(-z)) / (exp(z) + exp(-z))}$$\n","\n","    3. ## Sigmoid\n","      *  an activation function that is typically used in binary classification problems. It maps the input to a range between 0 and 1. The sigmoid function is a smoothed version of a step function, which allows it to produce a continuous output. \n","\n","      $$\\text{sigmoid(z) = 1 / (1 + exp(-z))}$$\n","**********************************\n","* ## **Prediction Error(Loss Function)**:\n"," discrepancy or difference between the predicted output or outcome and the actual or observed output. It measures how well a model's predictions align with the ground truth values, Examples:\n","\n"," 1. ## Cross Entropy\n","  $$\\text{CE = -sum(y_true * log(y_pred))} $$\n","\n","  2. ## Root Mean Square\n","    $$\\text{RMSE = sqrt(mean((y_true - y_pred)^2))} $$\n","\n","  3. ##  Mean Squared Error\n","    $$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n","\n","\n","\n"],"metadata":{"id":"YILJhLh-Mipr"}},{"cell_type":"markdown","source":["## Argmax\n","*   make the output value either 0 or 1, so 1 is for largest output value and 0 is for the rest of values.\n","*   easy to understand outputs.\n","*   it is not used in backpropagation, as it's $$ devrivative = 0 $$ \n","\n","\n","## Softmax\n","*   make the output value between 0 , 1, so largest output value will remain large afer SOFTMAX and also for other values.\n","*   it is used in backpropagation, as it's $$ devrivative \\neq 0 $$\n","\n","\n"],"metadata":{"id":"rI4hyDbNneIN"}},{"cell_type":"markdown","source":["## $$\\text {B. Steps} $$"],"metadata":{"id":"K5DwoNQd864r"}},{"cell_type":"markdown","source":["## 1- Split data into Training and Validation data\n","``` python\n","N = 1000 # number of samples\n","train_percent = 0.8\n","\n","x_train, y_train = x[:floor(N*train_percent), ], y[:floor(N*train_percent) ]\n","x_valid , y_valid = x[floor(N*train_percent):, ], y[floor(N*train_percent): ]\n","```\n","\n","## 2- Forward propagation (SOP) & Activation Function\n","``` python\n","def predict(x):\n","  a = x @ w.t() + b  # Forward\n","  return tc.softmax(a , dim =1) # Activation Function\n","\n","```\n","\n","## 3- Prediction Error\n","``` python\n","def loss_torch(x, y):\n","  y_hat = predict(x)\n","  return tc.mean(-(y*tc.log(y_hat)).sum(dim =1)) # cross entropy\n","```\n","## 4- Backpropagation & Optimization\n","``` python\n","EPOCHS = 1000\n","DELTA1 = 0.2 \n","DELTA2 = 0.1\n","train_ls , valid_ls = [], []\n","for _ in range(EPOCHS):\n","  curr_train_ls = loss_torch(x_train , y_train)\n","  curr_train_ls.backward()\n","  train_ls.append(curr_train_ls.data) \n","  # This creates a block of code where gradients are not tracked, \n","  # as we are not interested in computing gradients for the validation loss.\n","  with tc.no_grad():\n","    curr_valid_ls = loss_torch(x_valid , y_valid)\n","    valid_ls.append(curr_valid_ls.data)\n","    # Update weights and bias\n","    w -=  DELTA1 * w.grad.data  \n","    b -=  DELTA2 * b.grad.data\n","    # These reset the gradients to zero after each iteration \"inplace\".\n","    # This is necessary because PyTorch accumulates gradients by default.\n","    w.grad.zero_()\n","    b.grad.zero_()\n","```\n"],"metadata":{"id":"FSeI8TYYFBvq"}},{"cell_type":"markdown","source":["# ${\\color{salmon} {Lecture7: \\text {Automated-Differentiation}}}$"],"metadata":{"id":"RKY0GFgpRL45"}},{"cell_type":"markdown","source":["## New Function Example\n","$L = \\frac {1}{N} Î£(x_i-x_py_p)^2 . sin(x_p) . cos(y_pz_p)$\n","\n","* $ I_{x1} = x_i-x_py_p $\n","\n","* $I_{x2} = sin(x_p) $\n","\n","* $I_{x3} = cos(y_pz_p) $\n","\n","* $M_x = {I_{x1}}^2 $\n","\n","$L = I_{x3} [M_x . I_{x2}]$"],"metadata":{"id":"KMV-PRjl8oSa"}},{"cell_type":"markdown","source":["# Gradient & Backpropagation\n","\n","$\\frac{\\partial L}{\\partial x_p} = \\frac {I_{x3}} {N}$\n","$(M_x  \\frac{\\partial I_{x2}}{\\partial x_p} + I_{x2} \\frac{\\partial M_x}{\\partial x_p} )$\n","\n","* $\\frac{\\partial I_{x1}}{\\partial x_p} = -y_p$\n","\n","* $\\frac{\\partial I_{x2}}{\\partial x_p} = cos(x_p)$\n","\n","* $\\frac{\\partial M_x}{\\partial  I_{x1}} = 2I_{x1}$\n","\n","$\\frac{\\partial L}{\\partial x_p} = \\frac {cos(y_pz_p)}{N}$\n","$Î£[(x_i-x_py_p)^2 cos(x_p) + (-2y_p.(x_i-x_py_p)) sin(x_p)]$"],"metadata":{"id":"Cb4yXsOy_dcF"}},{"cell_type":"markdown","source":["## **OVERLOADING**"],"metadata":{"id":"68TalbzwxcTH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RZ5qbU_iu-ur"},"outputs":[],"source":["from math import sin, cos \n","from random import Random\n","from uuid import uuid4\n","\n","def generate_num(N=1):\n","    SEED = 5\n","    random_num = Random(x=SEED)\n","    return (\n","        [random_num.uniform(a=0, b=1) for _ in range(N)]\n","    )\n","\n","\n","class comp_node:\n","  def __init__(self, val, children =[] , op =\"assign\"):  # constructor\n","    self.val = val\n","    self.children = children\n","    self.grad = 0\n","    self.op = op\n","    self.backward_prop =  lambda : None\n","    self.identity = uuid4()\n","\n","  \"\"\"\n","  A private method that takes an object and returns it as a comp_node object if it is not already a comp_node.\n","  \"\"\"\n","  def __to_comp_node(self, obj):\n","      if not isinstance(obj , comp_node):\n","        return comp_node(val = obj)\n","      else:\n","        return obj \n","    \n","  ##### u ----> self , v ----> other\n","\n","  \"\"\"\n","  Overrides the - operator for subtraction. It returns a new comp_node object whose value is the difference of the two input nodes. \n","  It takes in the following parameters:\n","\n","  other : the node to subtract from the current node.\n","  \"\"\"\n","  def __sub__(self, other):\n","    other = self.__to_comp_node(other)\n","    out = comp_node(val = self.val - other.val, children=[self , other], op = \"sub\")\n","\n","    def __backward_prop():\n","      # d(u - v)/du = 1\n","      self.grad += out.grad * 1\n","      # d(u - v)/dv = -1\n","      other.grad += out.grad * -1\n","\n","    out.backward_prop = __backward_prop\n","    return out\n","      \n","  \"\"\"\n","  Overrides the - operator for subtraction when the left operand is not a comp_node.  \n","  It takes in the following parameters:\n","\n","  other : the node to subtract from.\n","  \"\"\"\n","  def __rsub__(self, other):\n","    other = self.__to_comp_node(other)\n","    return other - self\n","\n","  \"\"\"\n","  Overrides the + operator for addition. It returns a new comp_node object whose value is the sum of the two input nodes. \n","  It takes in the following parameters:\n","\n","  other : the node to add to the current node.\n","  \"\"\"\n","  def __add__(self, other):\n","    other = self.__to_comp_node(other)\n","    out = comp_node(val = self.val + other.val, children=[self , other], op = \"add\")\n","\n","    def __backward_prop():\n","      # d(u + v)/du = 1\n","      self.grad += out.grad * 1\n","      # d(u + v)/dv = 1\n","      other.grad += out.grad * 1\n","    out.backward_prop = __backward_prop\n","    return out\n","\n","  \"\"\"\n","  Overrides the + operator for addition when the left operand is not a comp_node. \n","  It returns a new comp_node object whose value is the sum of the two input nodes. \n","  It takes in the following parameters:\n","\n","  other : the node to add to.\n","  \"\"\"\n","  def __radd__(self, other):\n","    other = self.__to_comp_node(other)\n","    return other + self\n","\n","  \"\"\"\n","  Overrides the * operator for multiplication. \n","  It returns a new comp_node object whose value is the product of the two input nodes. \n","  It takes in the following parameters:\n","\n","  other : the node to multiply with the current node.\n","  \"\"\"\n","  def __mul__(self, other):\n","      other = self.__to_comp_node(other)\n","      out = comp_node(val=self.val * other.val, children=[self, other], op=\"mul\")\n","      def __backward_prop():\n","        \n","      # d(u * v)/du = v\n","       self.grad += out.grad * other.val\n","      # d(u * v)/dv = u\n","       other.grad += out.grad * self.val\n","      out.backward_prop = __backward_prop \n","      return out\n","\n","  \"\"\"\n","  Overrides the * operator for multiplication when the left operand is not a comp_node. \n","  It returns a new comp_node object whose value is the product of the two input nodes. \n","  It takes in the following parameters:\n","\n","  other : the node to multiply with.\n","  \"\"\"\n","  def __rmul__(self, other):\n","      return self.__mul__(other)\n","\n","  \"\"\"\n","  Overrides the ** operator for exponentiation. \n","  It returns a new comp_node object whose value is the current node raised to the power of the exponent.\n","  It takes in the following parameters:\n","\n","  exponent : the exponent to raise the current node to.\n","  \"\"\"\n","  def __pow__(self, exponent):\n","    if not isinstance(exponent, (int, float)):\n","      raise ValueError(\"type not supported\")\n","    out = comp_node(val = self.val ** exponent, children=[self], op = f\"pow {exponent}\")\n","\n","    def __backward_prop():\n","      # d(u^exp)/du = exp * u^(exp-1)\n","      self.grad += out.grad * (exponent * self.val**(exponent - 1))\n","    out.backward_prop = __backward_prop\n","    return out\n","\n","\n","    def __truediv__(self, other):\n","        other = self.__to_comp_node(other)\n","        if other.val == 0:\n","          raise ValueError(\"can not divide by zero\")\n","        out = comp_node(val=self.val / other.val, children=[self, other], op=\"div\")\n","        def __backward_prop():\n","          # d(u/v)/du = 1 / v \n","            self.grad += out.grad / other.val\n","          # d(u/v)/dv = -u/(v^2) \n","            other.grad -= out.grad * self.val / (other.val**2)\n","        out.backward_prop = __backward_prop\n","        return out\n","\n"]},{"cell_type":"code","source":["data_x = generate_num()\n","pnt_x , pnt_y, pnt_z = comp_node(val = 0.5), comp_node(val = 0.7), comp_node(val = 0.3)\n","\n","def loss_graph(pnt_x, pnt_y ,pnt_z, data_x):\n","  a_x = data_x * pnt_y\n","  i_x = pnt_x - a_x\n","  g_x = i_x**2\n","  m =  pnt_x.sin() \n","  p = g_x * m\n","  n = pnt_y * pnt_z\n","  o = (n).cos()\n","  l = o * p \n","  return l, [l, o, n , p , m , g_x, i_x, a_x, pnt_x, pnt_y, pnt_z]\n","\n","l, rev_topo_order = loss_graph(pnt_x, pnt_y,pnt_z, data_x[0])\n","rev_topo_order[0].grad = 1\n","\n","for i, node in enumerate(rev_topo_order):\n","  node.backward_prop()\n","  print(i, node)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rvpPyAXX3SwB","executionInfo":{"status":"ok","timestamp":1684069338225,"user_tz":-180,"elapsed":9,"user":{"displayName":"Mohammed AbuMandour","userId":"15885480873259056347"}},"outputId":"f5527f7d-d607-48fc-fd41-96c596739fb0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 comp_node(val= 0.00192 | children= 2 | op= mul | grad = 1.00000 )\n","1 comp_node(val= 0.97803 | children= 1 | op= cos | grad = 0.00196 )\n","2 comp_node(val= 0.21000 | children= 2 | op= mul | grad = -0.00041 )\n","3 comp_node(val= 0.00196 | children= 2 | op= mul | grad = 0.97803 )\n","4 comp_node(val= 0.47943 | children= 1 | op= sin | grad = 0.00400 )\n","5 comp_node(val= 0.00409 | children= 1 | op= pow 2 | grad = 0.46889 )\n","6 comp_node(val= 0.06397 | children= 2 | op= sub | grad = 0.05999 )\n","7 comp_node(val= 0.43603 | children= 2 | op= mul | grad = -0.05999 )\n","8 comp_node(val= 0.50000 | children= 0 | op= assignment | grad = 0.06350 )\n","9 comp_node(val= 0.70000 | children= 0 | op= assignment | grad = -0.03749 )\n","10 comp_node(val= 0.30000 | children= 0 | op= assignment | grad = -0.00029 )\n"]}]},{"cell_type":"markdown","source":["# ${\\color{salmon} {Lecture9: \\text {Graph Traversal}}}$"],"metadata":{"id":"m6-jRls7Svp4"}},{"cell_type":"code","source":["from graphviz import Digraph, Graph\n","'''\n","The function creates a new Digraph object using the Graphviz library and sets the format to SVG.\n","The direction of the edges is set to left-to-right using the rankdir attribute of the dot object.\n","\n","the function loops through each vertex in the vertices argument and adds it as a node to the graph using the node method of the dot object.\n","The name parameter specifies the name of the node,\n","which is converted to a string using str(node). \n","The label parameter specifies the label of the node, \n","which is set to the same value as the name using an f-string (f\"{node}\").\n","\n","the function loops through each key-value pair in the edges argument, \n","where the key is a node and the value is a list of adjacent nodes. \n","For each adjacent node in the list, \n","the function adds an edge between the two nodes using the edge method of the dot object.\n","\n","if there are multiple edges between the same two nodes,\n","only one of them will be added to the graph in the case of an undirected graph.\n","\n","'''\n","def draw_graph(vertices, edges, directed):\n","\n","    # Create a new Digraph\n","    if directed:\n","      dot = Digraph(format = 'svg')\n","    else:\n","      dot = Graph(format = 'svg')\n","\n","    # Set the direction of the edges to Top-to-Bottom\n","    dot.attr(rankdir='LR')\n","\n","    # Add nodes to the graph\n","    for node in vertices:\n","        dot.node(name= str(node), label= f\"{node}\")\n","    if not directed:\n","      #  keep track of edges that have already been added to the graph\n","      already_drawn = set()\n","\n","    # Add edges to the graph\n","    for node, edge_list in edges.items():\n","        for edge in edge_list:\n","          if directed:\n","            dot.edge(str(node), str(edge))\n","          else:\n","            #  ensure that the order of nodes in the edge does not affect the comparison of edges\n","            sorted_edges = tuple(sorted([node, edge]))\n","            #  If the edge has not been added, it adds the edge to the graph using the dot.edge() method \n","            # and adds the tuple to the already_drawn set\n","            if sorted_edges not in already_drawn:\n","              already_drawn.add(sorted_edges) \n","              dot.edge(str(node), str(edge))\n","\n","    # Render the graph to an SVG file and open it in the default viewer (optional)\n","    dot.render('graph', format='svg', view=True)\n","    return dot"],"metadata":{"id":"RJlG_r2ApOg_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. Breadth First Traversal"],"metadata":{"id":"baAvW_lkMgS3"}},{"cell_type":"code","source":["from collections import deque\n","# from collections import stack\n","\n","def bft(graph, start):\n","    # Create a queue for the nodes to be visited\n","    queue = deque([start])\n","    # Create a set to keep track of visited nodes\n","    visited = set()\n","    # Repeat until the queue is empty\n","    while queue:\n","        # Get the next node to visit\n","        node = queue.popleft()\n","        # If the node has not been visited, mark it as visited\n","        if node not in visited:\n","            visited.add(node)\n","            print(f\"visiting: {node}\")\n","            # Add all the neighbors of the node to the queue\n","            for neighbor in graph[node]:\n","                queue.append(neighbor)\n","\n","bft(graph1_edges, 1)"],"metadata":{"id":"PPjs4j6lEjfV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684422625467,"user_tz":-180,"elapsed":78,"user":{"displayName":"Muhamed El-Sayed","userId":"18011768401017469465"}},"outputId":"7c055b9c-14b8-4bd5-94e2-273431a55827"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["visiting: 1\n","visiting: 0\n","visiting: 5\n","visiting: 4\n","visiting: 2\n","visiting: 6\n","visiting: 7\n","visiting: 3\n"]}]},{"cell_type":"markdown","source":["## Depth First Search "],"metadata":{"id":"z4soBwOrc_zM"}},{"cell_type":"code","source":["def DFS(graph):\n","    visited = set()\n","\n","    def visit(node):\n","        if node in visited:\n","            return\n","        else:\n","            visited.add(node)\n","            neighbors = graph[node]\n","            print(f\"Visited node: {node}\")\n","            for neighbor in neighbors:\n","                visit(neighbor)\n","\n","    for node in graph.keys():\n","        if node not in visited:\n","            print(\"New DFS search\")\n","            visit(node)\n","\n","DFS(graph1_edges)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wApd7W4gVrux","executionInfo":{"status":"ok","timestamp":1684423004902,"user_tz":-180,"elapsed":13,"user":{"displayName":"Muhamed El-Sayed","userId":"18011768401017469465"}},"outputId":"693dfa7a-9a71-4930-ac9b-20031279cc44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["New DFS search\n","Visited node: 3\n","Visited node: 7\n","Visited node: 6\n","Visited node: 2\n","Visited node: 5\n","Visited node: 1\n","Visited node: 4\n","Visited node: 0\n"]}]},{"cell_type":"markdown","source":["## $\\text  {Topological sort} $"],"metadata":{"id":"qweYPvnbDA-h"}},{"cell_type":"code","source":["# Is not unique\n","def topo_sort(graph, start):\n","    stack = []\n","    visited = set()\n","\n","    def dfs(node):\n","        if node in visited:\n","            return\n","        else:\n","          visited.add(node)\n","          for neighbor in graph[node]:\n","              dfs(neighbor)\n","          stack.append(node)\n","\n","    dfs(start)\n","    return stack[::-1]\n","\n","\n","result = topo_sort(graph1_edges, 0)\n","print(\"Topological Sort Order:\", result)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Chc4eNWLC-bx","executionInfo":{"status":"ok","timestamp":1684422893032,"user_tz":-180,"elapsed":627,"user":{"displayName":"Muhamed El-Sayed","userId":"18011768401017469465"}},"outputId":"73d8dd2f-888a-4e2e-a3c7-d3501e0b9690"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Topological Sort Order: [0, 4, 1, 5, 2, 6, 7, 3]\n"]}]},{"cell_type":"markdown","source":["# Extra"],"metadata":{"id":"Yh_fmlaVGMBk"}},{"cell_type":"code","source":["graph_3_vert = set(list(range(8)))\n","graph_3_edges = {\n","   0: [1, 2, 3],        # Vertex 0 has edges to vertices 1, 2, and 3\n","    1: [4, 6],           # Vertex 1 has edges to vertices 4 and 6\n","    2: [5, 8],           # Vertex 2 has edges to vertices 5 and 8\n","    3: [7],              # Vertex 3 has an edge to vertex 7\n","    4: [5, 9, 11],       # Vertex 4 has edges to vertices 5, 9, and 11\n","    5: [6, 8],           # Vertex 5 has edges to vertices 6 and 8\n","    6: [7],              # Vertex 6 has an edge to vertex 7\n","    7: [10],             # Vertex 7 has an edge to vertex 10\n","    8: [10, 11],         # Vertex 8 has edges to vertices 10 and 11\n","    9: [11],             # Vertex 9 has an edge to vertex 11\n","    10: [],              # Vertex 10 has no outgoing edges\n","    11: []               # Vertex 11 has no outgoing edges\n","}\n","\n","draw_graph(graph_3_vert, graph_3_edges, True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":305},"id":"SBWk3h0LGPlN","executionInfo":{"status":"ok","timestamp":1686340779789,"user_tz":-180,"elapsed":596,"user":{"displayName":"Muhamed El-Sayed","userId":"18011768401017469465"}},"outputId":"575acc0b-372b-4aed-8c34-b9f3c9e38571"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"image/svg+xml":"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"602pt\" height=\"213pt\"\n viewBox=\"0.00 0.00 602.00 212.74\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 208.74)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-208.74 598,-208.74 598,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-100.92\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-97.22\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"117\" cy=\"-100.92\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"117\" y=\"-97.22\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M54.4,-100.92C62.39,-100.92 71.31,-100.92 79.82,-100.92\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"79.92,-104.42 89.92,-100.92 79.92,-97.42 79.92,-104.42\"/>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"207\" cy=\"-173.92\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"207\" y=\"-170.22\" font-family=\"Times,serif\" font-size=\"14.00\">2</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50.02,-110.72C61.83,-115.92 76.67,-122.37 90,-127.92 118.18,-139.64 150.47,-152.39 173.82,-161.49\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"172.65,-164.79 183.24,-165.15 175.19,-158.26 172.65,-164.79\"/>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"117\" cy=\"-27.92\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"117\" y=\"-24.22\" font-family=\"Times,serif\" font-size=\"14.00\">3</text>\n</g>\n<!-- 0&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>0&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M44.55,-87.19C57.74,-76.24 76.41,-60.76 91.36,-48.36\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"93.76,-50.91 99.22,-41.83 89.29,-45.52 93.76,-50.91\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"207\" cy=\"-110.92\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"207\" y=\"-107.22\" font-family=\"Times,serif\" font-size=\"14.00\">4</text>\n</g>\n<!-- 1&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>1&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M143.93,-103.86C152.16,-104.8 161.42,-105.85 170.22,-106.85\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"169.93,-110.34 180.26,-107.99 170.72,-103.38 169.93,-110.34\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"387\" cy=\"-35.92\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"387\" y=\"-32.22\" font-family=\"Times,serif\" font-size=\"14.00\">6</text>\n</g>\n<!-- 1&#45;&gt;6 -->\n<g id=\"edge5\" class=\"edge\">\n<title>1&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M142.09,-94.08C153.54,-90.89 167.45,-87.1 180,-83.92 240.05,-68.69 310.42,-52.69 351.42,-43.54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"352.19,-46.96 361.19,-41.37 350.67,-40.13 352.19,-46.96\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"297\" cy=\"-173.92\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"297\" y=\"-170.22\" font-family=\"Times,serif\" font-size=\"14.00\">5</text>\n</g>\n<!-- 2&#45;&gt;5 -->\n<g id=\"edge6\" class=\"edge\">\n<title>2&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M234.4,-173.92C242.39,-173.92 251.31,-173.92 259.82,-173.92\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"259.92,-177.42 269.92,-173.92 259.92,-170.42 259.92,-177.42\"/>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"387\" cy=\"-171.92\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"387\" y=\"-168.22\" font-family=\"Times,serif\" font-size=\"14.00\">8</text>\n</g>\n<!-- 2&#45;&gt;8 -->\n<g id=\"edge7\" class=\"edge\">\n<title>2&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M228.24,-185.26C240.03,-191.21 255.4,-197.88 270,-200.92 293.5,-205.81 300.57,-206.12 324,-200.92 335.46,-198.37 347.3,-193.47 357.53,-188.42\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"359.16,-191.51 366.41,-183.79 355.93,-185.3 359.16,-191.51\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"477\" cy=\"-35.92\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"477\" y=\"-32.22\" font-family=\"Times,serif\" font-size=\"14.00\">7</text>\n</g>\n<!-- 3&#45;&gt;7 -->\n<g id=\"edge8\" class=\"edge\">\n<title>3&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M142.87,-21.88C194.17,-10.41 315.19,11.66 414,-8.92 425.06,-11.22 436.57,-15.61 446.62,-20.19\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"445.23,-23.4 455.76,-24.57 448.26,-17.09 445.23,-23.4\"/>\n</g>\n<!-- 4&#45;&gt;5 -->\n<g id=\"edge9\" class=\"edge\">\n<title>4&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M226.23,-123.97C238.74,-132.92 255.54,-144.95 269.51,-154.96\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"267.62,-157.9 277.79,-160.88 271.7,-152.21 267.62,-157.9\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"297\" cy=\"-119.92\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"297\" y=\"-116.22\" font-family=\"Times,serif\" font-size=\"14.00\">9</text>\n</g>\n<!-- 4&#45;&gt;9 -->\n<g id=\"edge10\" class=\"edge\">\n<title>4&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M233.93,-113.57C242.16,-114.41 251.42,-115.36 260.22,-116.26\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"259.95,-119.75 270.26,-117.28 260.67,-112.78 259.95,-119.75\"/>\n</g>\n<!-- 11 -->\n<g id=\"node11\" class=\"node\">\n<title>11</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"477\" cy=\"-119.92\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"477\" y=\"-116.22\" font-family=\"Times,serif\" font-size=\"14.00\">11</text>\n</g>\n<!-- 4&#45;&gt;11 -->\n<g id=\"edge11\" class=\"edge\">\n<title>4&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M231.21,-102.49C242.71,-98.75 256.91,-94.78 270,-92.92 330.74,-84.28 401.7,-99.04 442.49,-109.92\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"441.62,-113.31 452.19,-112.58 443.47,-106.56 441.62,-113.31\"/>\n</g>\n<!-- 5&#45;&gt;6 -->\n<g id=\"edge12\" class=\"edge\">\n<title>5&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M313.3,-159.39C317.01,-155.51 320.82,-151.2 324,-146.92 344.01,-119.92 362.15,-85.77 373.57,-62.53\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"376.83,-63.82 378.03,-53.29 370.53,-60.77 376.83,-63.82\"/>\n</g>\n<!-- 5&#45;&gt;8 -->\n<g id=\"edge13\" class=\"edge\">\n<title>5&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M324.4,-173.32C332.39,-173.13 341.31,-172.93 349.82,-172.74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"350,-176.24 359.92,-172.51 349.84,-169.24 350,-176.24\"/>\n</g>\n<!-- 6&#45;&gt;7 -->\n<g id=\"edge14\" class=\"edge\">\n<title>6&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M414.4,-35.92C422.39,-35.92 431.31,-35.92 439.82,-35.92\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"439.92,-39.42 449.92,-35.92 439.92,-32.42 439.92,-39.42\"/>\n</g>\n<!-- 10 -->\n<g id=\"node12\" class=\"node\">\n<title>10</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"567\" cy=\"-100.92\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"567\" y=\"-97.22\" font-family=\"Times,serif\" font-size=\"14.00\">10</text>\n</g>\n<!-- 7&#45;&gt;10 -->\n<g id=\"edge15\" class=\"edge\">\n<title>7&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M495.81,-49.07C508.45,-58.41 525.63,-71.1 539.83,-81.59\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"538.1,-84.66 548.22,-87.78 542.25,-79.03 538.1,-84.66\"/>\n</g>\n<!-- 8&#45;&gt;11 -->\n<g id=\"edge17\" class=\"edge\">\n<title>8&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M407.97,-160.12C419.56,-153.27 434.35,-144.52 447.19,-136.94\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"449.28,-139.77 456.11,-131.67 445.72,-133.74 449.28,-139.77\"/>\n</g>\n<!-- 8&#45;&gt;10 -->\n<g id=\"edge16\" class=\"edge\">\n<title>8&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M413.87,-169.52C438.05,-166.56 474.72,-160.18 504,-146.92 517.91,-140.61 531.72,-130.81 542.77,-121.87\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"545.22,-124.38 550.63,-115.27 540.72,-119.02 545.22,-124.38\"/>\n</g>\n<!-- 9&#45;&gt;11 -->\n<g id=\"edge18\" class=\"edge\">\n<title>9&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M324.26,-119.92C354.85,-119.92 405.55,-119.92 439.84,-119.92\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"439.92,-123.42 449.92,-119.92 439.92,-116.42 439.92,-123.42\"/>\n</g>\n</g>\n</svg>\n","text/plain":["<graphviz.graphs.Digraph at 0x7f6379f5ed70>"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["print(\"output of DFS on graph 3\")\n","DFS(graph_3_edges, 0) \n","print('*'*50)\n","\n","print(\"output of BFT on graph 3\")\n","bft(graph_3_edges, 0)\n","print('*'*50)\n","\n","print(\"output of topt sort on graph 3\")\n","result = topo_sort(graph_3_edges, 0)\n","print(\"Topological Sort Order:\", result)\n","print('*'*50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vI_FozfvHSVd","executionInfo":{"status":"ok","timestamp":1684422625477,"user_tz":-180,"elapsed":69,"user":{"displayName":"Muhamed El-Sayed","userId":"18011768401017469465"}},"outputId":"956ae33f-5315-4363-e796-e788ee0e6b38"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["output of DFS on graph 3\n","New DFS search\n","Visited node: 10\n","Visited node: 7\n","Visited node: 6\n","Visited node: 11\n","Visited node: 8\n","Visited node: 5\n","Visited node: 9\n","Visited node: 4\n","Visited node: 1\n","Visited node: 2\n","Visited node: 3\n","Visited node: 0\n","**************************************************\n","output of BFT on graph 3\n","visiting: 0\n","visiting: 1\n","visiting: 2\n","visiting: 3\n","visiting: 4\n","visiting: 6\n","visiting: 5\n","visiting: 8\n","visiting: 7\n","visiting: 9\n","visiting: 11\n","visiting: 10\n","**************************************************\n","output of topt sort on graph 3\n","Topological Sort Order: [0, 3, 2, 1, 4, 9, 5, 8, 11, 6, 7, 10]\n","**************************************************\n"]}]},{"cell_type":"markdown","source":["# ADJACENCY MATRIX"],"metadata":{"id":"qSEg2rtATgo_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GoNj8iGTsaKN"},"outputs":[],"source":["adj_matrix_old = [\n","    [0, 1, 0, 0, 1, 0, 0, 0],\n","    [1, 0, 0, 0, 0, 1, 0, 0],\n","    [0, 0, 0, 0, 0, 1, 1, 0],\n","    [0, 0, 0, 0, 0, 0, 0, 1],\n","    [1, 0, 0, 0, 0, 0, 0, 0],\n","    [0, 1, 1, 0, 0, 0, 1, 0],\n","    [0, 0, 1, 0, 0, 1, 0, 1],\n","    [0, 0, 0, 1, 0, 0, 1, 0]\n","]"]},{"cell_type":"markdown","source":["# BFT"],"metadata":{"id":"LLHog2sPs3WI"}},{"cell_type":"code","source":["from collections import deque\n","\n","def bft(adj_matrix, start):\n","  # assumes that the adjacency matrix is a square matrix\n","    num_nodes = len(adj_matrix)\n","    # Create a queue for the nodes to be visited\n","    queue = deque([start])\n","    # Create a set to keep track of visited nodes\n","    visited = set()\n","    # Repeat until the queue is empty\n","    while queue:\n","        # Get the next node to visit\n","        node = queue.popleft()\n","        # If the node has not been visited, mark it as visited\n","        if node not in visited:\n","            visited.add(node)\n","            print(f\"Visiting node: {node}\")\n","            # iterates over the row corresponding to the current node in the adjacency matrix. \n","            # It checks if the value in the matrix is 1, indicating an edge between the current node and the node at that index. \n","            # If an edge exists, it adds the index to the neighbors list.\n","            neighbors = [i for i in range(num_nodes) if adj_matrix[node][i] == 1]\n","            # Add all the neighbors of the node to the queue\n","            for neighbor in neighbors:\n","                queue.append(neighbor)\n","bft(adj_matrix_old, 1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bqjUt-dms3CJ","executionInfo":{"status":"ok","timestamp":1684306048181,"user_tz":-180,"elapsed":22,"user":{"displayName":"Muhamed El-Sayed","userId":"18011768401017469465"}},"outputId":"a7bf2f0c-ff53-4691-e305-6e60c1273d88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Visiting node: 1\n","Visiting node: 0\n","Visiting node: 5\n","Visiting node: 4\n","Visiting node: 2\n","Visiting node: 6\n","Visiting node: 7\n","Visiting node: 3\n"]}]},{"cell_type":"markdown","source":["# DFS"],"metadata":{"id":"btVXzMd9uPUr"}},{"cell_type":"code","source":["visited = set()  # Create an empty set to store the visited nodes\n","\n","def DFS(node, adj_matrix):\n","    if node not in visited:  # Check if the node has not been visited before\n","        visited.add(node)  # Add the node to the visited set\n","        print(f\"Visiting node: {node}\")  # Print the visited node\n","        for neighbor in range(len(adj_matrix[node])):\n","            if adj_matrix[node][neighbor] == 1:  # Check if there is an edge between the current node and its neighbor\n","                DFS(neighbor, adj_matrix)  # Call the DFS function recursively with the current neighbor as input\n","\n","\n","for node in range(len(adj_matrix_old)):\n","    if node not in visited:  # Check if the node has not been visited before\n","        print(\"New DFS search\")  # Print a message indicating the start of a new DFS search\n","        DFS(node, adj_matrix_old)  # Call the DFS function with the current node and the adjacency matrix as inputs\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NZSCCLHLuP7g","executionInfo":{"status":"ok","timestamp":1684306048182,"user_tz":-180,"elapsed":20,"user":{"displayName":"Muhamed El-Sayed","userId":"18011768401017469465"}},"outputId":"102ba200-2b7b-473f-ddc8-150cab5176db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["New DFS search\n","Visiting node: 0\n","Visiting node: 1\n","Visiting node: 5\n","Visiting node: 2\n","Visiting node: 6\n","Visiting node: 7\n","Visiting node: 3\n","Visiting node: 4\n"]}]},{"cell_type":"markdown","source":["# Topo_Sort"],"metadata":{"id":"fcADfmr6mxFh"}},{"cell_type":"code","source":["def topo_sort_mat(adj_matrix, start):\n","    stack = []\n","    visited = set()\n","\n","    def dfs(node):\n","        visited.add(node)\n","        for neighbor in range(len(adj_matrix)):\n","            if adj_matrix[node][neighbor] == 1 and neighbor not in visited:\n","                dfs(neighbor)\n","        stack.append(node)\n","\n","    dfs(start)\n","    return stack[::-1]\n","\n","\n","mat = [\n","    [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n","    [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n","    [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n","    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n","    [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1],\n","    [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n","    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n","    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n","    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n","    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n","    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","]\n","\n","topo_sort_mat(mat, 0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5MfOWs6qm00z","executionInfo":{"status":"ok","timestamp":1684306048182,"user_tz":-180,"elapsed":19,"user":{"displayName":"Muhamed El-Sayed","userId":"18011768401017469465"}},"outputId":"82942dda-4d0f-40a5-a067-0e6ec2e37f32"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 3, 2, 1, 4, 9, 5, 8, 11, 6, 7, 10]"]},"metadata":{},"execution_count":4}]}]}